{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98af9b61",
   "metadata": {},
   "source": [
    "# üîç Sustainable Prompt Optimization - NLP Scoring Module\n",
    "\n",
    "This notebook reuses and adapts code from the **\"NLP Pipeline + Probabilistic Language Models\" lab** to evaluate the **linguistic fluency** of prompt alternatives. It implements:\n",
    "- Tokenization, Normalization, Stopword Removal (Lab Part 1)\n",
    "- Unigram and Bigram Probabilistic Language Models (Lab Part 2)\n",
    "These models will assign fluency scores to optimized prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20322f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yogeshkumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yogeshkumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# üì¶ Setup and Imports (from Lab Step 1)\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download needed NLTK components\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af30ba",
   "metadata": {},
   "source": [
    "### üß™ Tokenization & Normalization (Lab Step 3 & 4)\n",
    "\n",
    "This step normalizes prompt text using:\n",
    "- Lowercasing\n",
    "- Regex-based tokenizer\n",
    "- Stopword removal\n",
    "- Porter stemming\n",
    "\n",
    "‚≠ï **Lab Connection**: This pipeline was implemented as preprocessing in `Step 3‚Äì4` of the lab notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5644b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab-style tokenizer and normalizer\n",
    "def simple_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "def normalize(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in tokens if word not in stop_words and word not in string.punctuation]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e546d9",
   "metadata": {},
   "source": [
    "### üìä Unigram Model (Lab Part 2 ‚Äì Section: Unigram Model)\n",
    "\n",
    "Computes P(w·µ¢) = count(w·µ¢) / N for each word. Used here to score prompt fluency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae2442c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example corpus to build unigram model - replace with domain corpus if available\n",
    "sample_corpus = \"How to configure server settings quickly and securely. Configure system with minimal steps for fast deployment.\"\n",
    "tokens = normalize(simple_tokenizer(sample_corpus))\n",
    "unigram_counts = Counter(tokens)\n",
    "total_words = len(tokens)\n",
    "\n",
    "def unigram_prob(word):\n",
    "    return unigram_counts[word] / total_words if word in unigram_counts else 1e-7  # small smoothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4ec3d3",
   "metadata": {},
   "source": [
    "### üîó Unigram Sentence Probability (Lab Part 2 ‚Äì \"Chain Rule with Unigrams\")\n",
    "\n",
    "Multiplies P(w‚ÇÅ) * P(w‚ÇÇ) * ... to give fluency score of full prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "344c07ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_prob_unigram(sentence):\n",
    "    words = normalize(simple_tokenizer(sentence))\n",
    "    prob = 1.0\n",
    "    for word in words:\n",
    "        prob *= unigram_prob(word)\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921169e1",
   "metadata": {},
   "source": [
    "### üß© Bigram Model (Lab Part 2 ‚Äì Section: Bigram Model)\n",
    "\n",
    "Captures adjacent word dependencies. Requires Count(w·µ¢‚Çã‚ÇÅ, w·µ¢) and Count(w·µ¢‚Çã‚ÇÅ).\n",
    "\n",
    "‚≠ï **Lab Connection**: Matching Section \"Bigram Model with MLE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ca97ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build bigram counts on same corpus\n",
    "bigram_counts = defaultdict(int)\n",
    "for i in range(len(tokens) - 1):\n",
    "    pair = (tokens[i], tokens[i + 1])\n",
    "    bigram_counts[pair] += 1\n",
    "\n",
    "def bigram_prob(w1, w2):\n",
    "    return bigram_counts[(w1, w2)] / unigram_counts[w1] if unigram_counts[w1] > 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58f8f2a",
   "metadata": {},
   "source": [
    "### üß™ Bigram Sentence Probability (Lab Part 2 ‚Äì Sentence Probability Using Bigrams)\n",
    "\n",
    "Uses the bigram chain rule:\n",
    "P(w‚ÇÅ) ¬∑ P(w‚ÇÇ|w‚ÇÅ) ¬∑ P(w‚ÇÉ|w‚ÇÇ)...\n",
    "Score is used to select fluent prompt variants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7ef5a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_prob_bigram(sentence):\n",
    "    words = normalize(simple_tokenizer(sentence))\n",
    "    if not words: return 0\n",
    "    prob = unigram_prob(words[0])  # start with P(w1)\n",
    "    for i in range(len(words) - 1):\n",
    "        prob *= bigram_prob(words[i], words[i + 1])\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646b670",
   "metadata": {},
   "source": [
    "### ‚úÖ Example: Evaluate Prompt Alternatives\n",
    "\n",
    "Select prompt candidates and compare their fluency scores using Unigram/Bigram models.\n",
    "\n",
    "‚≠ï **Lab Connection**: This operationalizes the sentence scoring shown in `sentence_prob_unigram` and `sentence_prob_bigram` functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "133f0701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fluency Scores (Unigram and Bigram) ---\n",
      "Prompt: \"How to configure system quickly.\"\n",
      " ‚§∑ Unigram: 1.50e-03, Bigram: 0.00e+00\n",
      "\n",
      "Prompt: \"Please provide configuration steps in a fast manner.\"\n",
      " ‚§∑ Unigram: 1.50e-24, Bigram: 0.00e+00\n",
      "\n",
      "Prompt: \"Assist with server setup guidance to deploy with speed.\"\n",
      " ‚§∑ Unigram: 8.26e-31, Bigram: 0.00e+00\n",
      "\n",
      "Prompt: \"Fast deployment via configuration help.\"\n",
      " ‚§∑ Unigram: 1.50e-17, Bigram: 0.00e+00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "candidates = [\n",
    "    \"How to configure system quickly.\",\n",
    "    \"Please provide configuration steps in a fast manner.\",\n",
    "    \"Assist with server setup guidance to deploy with speed.\",\n",
    "    \"Fast deployment via configuration help.\"\n",
    "]\n",
    "\n",
    "print(\"--- Fluency Scores (Unigram and Bigram) ---\")\n",
    "for prompt in candidates:\n",
    "    u_score = sentence_prob_unigram(prompt)\n",
    "    b_score = sentence_prob_bigram(prompt)\n",
    "    print(f\"Prompt: \\\"{prompt}\\\"\\n ‚§∑ Unigram: {u_score:.2e}, Bigram: {b_score:.2e}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c378bea",
   "metadata": {},
   "source": [
    "### ‚úÖ Implementing Combined Sentence Probability Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f53161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_sentence_prob(sentence, lambda_weight=0.7):\n",
    "    \"\"\"\n",
    "    Combine bigram and unigram scores using interpolation.\n",
    "    \"\"\"\n",
    "    words = normalize(simple_tokenizer(sentence))\n",
    "    if not words:\n",
    "        return 0.0\n",
    "    prob = unigram_prob(words[0])  # start with unigram prob for first word\n",
    "\n",
    "    for i in range(1, len(words)):\n",
    "        p_bigram = bigram_prob(words[i - 1], words[i])\n",
    "        p_unigram = unigram_prob(words[i])\n",
    "        interpolated = lambda_weight * p_bigram + (1.0 - lambda_weight) * p_unigram\n",
    "        prob *= interpolated\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd4f72b",
   "metadata": {},
   "source": [
    "### Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e2b2da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_prompt_combined(prompts, lambda_weight=0.7):\n",
    "    scores = []\n",
    "    for prompt in prompts:\n",
    "        score = combined_sentence_prob(prompt, lambda_weight)\n",
    "        scores.append((prompt, score))\n",
    "    best = max(scores, key=lambda x: x[1])\n",
    "    return {\n",
    "        'best_prompt': best[0],\n",
    "        'scores': sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad5542b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Best prompt: How to configure system quickly.\n",
      "\"How to configure system quickly.\": combined score = 1.87e-03\n",
      "\"Fast deployment via configuration help.\": combined score = 3.25e-18\n",
      "\"Please provide configuration steps in a fast manner.\": combined score = 9.74e-26\n",
      "\"Assist with server setup guidance to deploy with speed.\": combined score = 2.01e-33\n"
     ]
    }
   ],
   "source": [
    "results = select_best_prompt_combined(candidates, lambda_weight=0.7)\n",
    "print(\"‚úÖ Best prompt:\", results['best_prompt'])\n",
    "for p, s in results['scores']:\n",
    "    print(f'\"{p}\": combined score = {s:.2e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7bbbea",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "This probabilistic scoring component will be combined with:\n",
    "- Cosine similarity from embedding models (to enforce semantic equivalence)\n",
    "- Token length/FLOP analysis (to measure inference cost)\n",
    "\n",
    "Language models help **optimize prompt clarity** without sacrificing computational efficiency.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
